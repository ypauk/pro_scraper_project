import asyncio
import random
from src.client import BrowserClient
from src.parser import QuoteParser
from src.models import QuoteModel
from loguru import logger
from src.utils import human_delay, smooth_scroll, human_mouse_move
from src.settings import BASE_DELAY, CONCURRENCY, PROXY_LIST
from src.state_manager import StateManager
from src.exporter import Exporter


class Scraper:
    def __init__(self, max_items: int = 50, proxy: dict = None, concurrency: int = CONCURRENCY):
        self.client = BrowserClient(proxy=proxy)
        self.parser = QuoteParser()
        self.max_items = max_items
        self.concurrency = concurrency
        self.results: list[QuoteModel] = []
        self._lock = asyncio.Lock()
        self.state_manager = StateManager()

    async def scrape_page(self, url: str, index: int) -> str | None:
        if len(self.results) >= self.max_items:
            return None

        current_proxy = PROXY_LIST[index % len(PROXY_LIST)] if PROXY_LIST else None
        current_ua = self.client.get_random_ua()

        context = await self.client.browser.new_context(
            user_agent=current_ua,
            proxy=current_proxy
        )
        page = await context.new_page()

        try:
            # –¢—É—Ç –º–∏ –≤–ø–µ–≤–Ω–µ–Ω—ñ, —â–æ url - —Ü–µ —Ä—è–¥–æ–∫
            logger.info(f"üöÄ [–°—Ç–æ—Ä—ñ–Ω–∫–∞ #{index}] –ü–µ—Ä–µ—Ö—ñ–¥: {url}")
            await page.goto(url, wait_until="domcontentloaded", timeout=60000)

            if random.random() < 0.8: await human_mouse_move(page)
            if random.random() < 0.6:
                await smooth_scroll(page)
                await human_mouse_move(page)

            new_items = await self.parser.parse_quotes(page)
            next_page_url = await self.parser.get_next_page_url(page)

            async with self._lock:
                self._update_results(new_items)
                count = len(self.results)

            logger.success(f"‚úÖ [–°—Ç–æ—Ä—ñ–Ω–∫–∞ #{index}] –ó—ñ–±—Ä–∞–Ω–æ {len(new_items)} —à—Ç. (–†–∞–∑–æ–º: {count})")

            if next_page_url:
                self.state_manager.save_checkpoint(next_page_url)

            await human_delay(BASE_DELAY[0], BASE_DELAY[1])
            return next_page_url

        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ #{index}: {e}")
            return "ERROR_SIGNAL"
        finally:
            await context.close()

    async def run(self, start_url: str):
        await self.client.start()

        # --- –í–ò–ü–†–ê–í–õ–ï–ù–ò–ô –ë–õ–û–ö –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø ---
        checkpoint_data = self.state_manager.load_checkpoint()

        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏—Ö –¥–∞–Ω–∏—Ö
        if isinstance(checkpoint_data, dict):
            current_url = checkpoint_data.get("last_url", start_url)
        elif isinstance(checkpoint_data, str):
            current_url = checkpoint_data
        else:
            current_url = start_url

        if current_url != start_url:
            logger.info(f"‚ôªÔ∏è –í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –∑ —á–µ–∫–ø–æ—ó–Ω—Ç–∞: {current_url}")
        # ---------------------------------------

        page_index = 1

        try:
            while current_url and len(self.results) < self.max_items:
                # –ü–µ—Ä–µ–¥–∞—î–º–æ –≤ scrape_page –≤–∂–µ –≥–∞—Ä–∞–Ω—Ç–æ–≤–∞–Ω–æ —á–∏—Å—Ç–∏–π URL (—Ä—è–¥–æ–∫)
                result = await self.scrape_page(current_url, page_index)

                if result == "ERROR_SIGNAL":
                    logger.warning(f"‚ö†Ô∏è –ü–µ—Ä–µ—Ä–∏–≤–∞–Ω–Ω—è —á–µ—Ä–µ–∑ –∑–±—ñ–π. –ß–µ–∫–ø–æ—ó–Ω—Ç –∑–∞–ª–∏—à–∏–≤—Å—è –Ω–∞: {current_url}")
                    break

                current_url = result
                page_index += 1

            if current_url is None and len(self.results) < self.max_items:
                logger.info("üèÅ –°–∞–π—Ç –∑–∞–∫—ñ–Ω—á–∏–≤—Å—è.")
                self.state_manager.clear_checkpoint()
            elif len(self.results) >= self.max_items:
                logger.info(f"üéØ –õ—ñ–º—ñ—Ç —É {self.max_items} –¥–æ—Å—è–≥–Ω—É—Ç–æ.")
                self.state_manager.clear_checkpoint()

            return self.results

        except Exception as e:
            logger.critical(f"üí• –ö—Ä–∏—Ç–∏—á–Ω–∏–π –∑–±—ñ–π: {e}")
            return self.results
        finally:
            await self.client.stop()

    def _update_results(self, new_items: list[QuoteModel]):
        existing_texts = {item.text for item in self.results}
        for item in new_items:
            if item.text not in existing_texts and len(self.results) < self.max_items:
                self.results.append(item)
                Exporter.append_to_csv(item, filename="live_results.csv")