import asyncio
import random
from src.client import BrowserClient
from src.parser import QuoteParser
from src.models import QuoteModel
from loguru import logger
from src.utils import human_delay, smooth_scroll, human_mouse_move
from src.settings import BASE_DELAY, CONCURRENCY, PROXY_LIST


class Scraper:
    def __init__(self, max_items: int = 50, proxy: dict = None, concurrency: int = CONCURRENCY):
        self.client = BrowserClient(proxy=proxy)
        self.parser = QuoteParser()
        self.max_items = max_items
        self.concurrency = concurrency
        self.results: list[QuoteModel] = []
        self._lock = asyncio.Lock()

    async def scrape_page(self, url: str, index: int) -> str | None:
        """
        –û–±—Ä–æ–±–ª—è—î —Å—Ç–æ—Ä—ñ–Ω–∫—É —Ç–∞ –ü–û–í–ï–†–¢–ê–Ñ URL –Ω–∞—Å—Ç—É–ø–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏, —è–∫—â–æ –≤—ñ–Ω —î.
        """
        if len(self.results) >= self.max_items:
            return None

        # 1. –í–∏–±—ñ—Ä –ø—Ä–æ–∫—Å—ñ —Ç–∞ UA
        current_proxy = PROXY_LIST[index % len(PROXY_LIST)] if PROXY_LIST else None
        current_ua = self.client.get_random_ua()

        context = await self.client.browser.new_context(
            user_agent=current_ua,
            proxy=current_proxy
        )
        page = await context.new_page()
        next_page_url = None

        try:
            logger.info(f"üöÄ [–°—Ç–æ—Ä—ñ–Ω–∫–∞ #{index}] –ü–µ—Ä–µ—Ö—ñ–¥: {url}")
            await page.goto(url, wait_until="domcontentloaded", timeout=60000)

            # --- –ï–ú–£–õ–Ø–¶–Ü–Ø ---
            if random.random() < 0.8: await human_mouse_move(page)
            if random.random() < 0.6:
                await smooth_scroll(page)
                await human_mouse_move(page)

            # --- –ü–ê–†–°–ò–ù–ì –î–ê–ù–ò–• ---
            new_items = await self.parser.parse_quotes(page)

            # --- –ü–û–®–£–ö –ù–ê–°–¢–£–ü–ù–û–á –°–¢–û–†–Ü–ù–ö–ò (–í–∞—Ä—ñ–∞–Ω—Ç –í) ---
            next_page_url = await self.parser.get_next_page_url(page)

            async with self._lock:
                self._update_results(new_items)
                count = len(self.results)

            logger.success(f"‚úÖ [–°—Ç–æ—Ä—ñ–Ω–∫–∞ #{index}] –ó—ñ–±—Ä–∞–Ω–æ {len(new_items)} —à—Ç. (–†–∞–∑–æ–º: {count})")

            await human_delay(BASE_DELAY[0], BASE_DELAY[1])

        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ #{index}: {e}")
        finally:
            await context.close()
            return next_page_url

    async def run(self, start_url: str):
        """
        –¢–æ—á–∫–∞ –≤—Ö–æ–¥—É –¥–ª—è Crawler. –ô–¥–µ –ø–æ –∫–Ω–æ–ø–∫–∞—Ö 'Next'.
        """
        await self.client.start()
        current_url = start_url
        page_index = 1

        try:
            # –ü—Ä–∞—Ü—é—î–º–æ, –ø–æ–∫–∏ —î –ø–æ—Å–∏–ª–∞–Ω–Ω—è —ñ –º–∏ –Ω–µ –Ω–∞–±—Ä–∞–ª–∏ –ª—ñ–º—ñ—Ç
            while current_url and len(self.results) < self.max_items:
                # –í–∏–∫–ª–∏–∫–∞—î–º–æ –æ–±—Ä–æ–±–∫—É —ñ –æ—Ç—Ä–∏–º—É—î–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –º–∞–π–±—É—Ç–Ω—é —Å—Ç–æ—Ä—ñ–Ω–∫—É
                current_url = await self.scrape_page(current_url, page_index)
                page_index += 1

                if not current_url:
                    logger.info("üèÅ –ö–Ω–æ–ø–∫–∞ 'Next' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞ –∞–±–æ –ª—ñ–º—ñ—Ç –¥–æ—Å—è–≥–Ω—É—Ç–æ. –ó—É–ø–∏–Ω—è—é—Å—å.")

            logger.info(f"üèÅ –ö—Ä–∞—É–ª—ñ–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –í—Å—å–æ–≥–æ –∑—ñ–±—Ä–∞–Ω–æ: {len(self.results)}")
            return self.results
        finally:
            await self.client.stop()

    def _update_results(self, new_items: list[QuoteModel]):
        existing_texts = {item.text for item in self.results}
        for item in new_items:
            if item.text not in existing_texts and len(self.results) < self.max_items:
                self.results.append(item)