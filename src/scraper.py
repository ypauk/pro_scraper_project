import asyncio
import random
from src.client import BrowserClient
from src.parser import QuoteParser
from src.models import QuoteModel
from loguru import logger
# –î–æ–¥–∞—î–º–æ –Ω–æ–≤—ñ —É—Ç–∏–ª—ñ—Ç–∏ –¥–ª—è —ñ–º—ñ—Ç–∞—Ü—ñ—ó –ª—é–¥–∏–Ω–∏
from src.utils import human_delay, smooth_scroll, human_mouse_move
from src.settings import BASE_DELAY, CONCURRENCY, PROXY_LIST


class Scraper:
    def __init__(self, max_items: int = 50, proxy: dict = None, concurrency: int = CONCURRENCY):
        self.client = BrowserClient(proxy=proxy)
        self.parser = QuoteParser()
        self.max_items = max_items
        self.concurrency = concurrency
        self.results: list[QuoteModel] = []
        self._lock = asyncio.Lock()

    async def scrape_page(self, semaphore: asyncio.Semaphore, url: str, index: int):
        """
        –û–±—Ä–æ–±–∫–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –∑ —ñ–º—ñ—Ç–∞—Ü—ñ—î—é —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ (—Å–∫—Ä–æ–ª, –º–∏—à–∞, –ø–∞—É–∑–∏)
        """
        async with semaphore:
            if len(self.results) >= self.max_items:
                return

            # 1. –†–æ—Ç–∞—Ü—ñ—è –ø—Ä–æ–∫—Å—ñ
            current_proxy = None
            if PROXY_LIST:
                current_proxy = PROXY_LIST[index % len(PROXY_LIST)]
                proxy_label = current_proxy.get('server', 'unknown')
            else:
                proxy_label = "–†—ñ–¥–Ω–∏–π IP"

            # 2. –£–Ω—ñ–∫–∞–ª—å–Ω–∏–π User-Agent
            current_ua = self.client.get_random_ua()

            # 3. –Ü–∑–æ–ª—å–æ–≤–∞–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            context = await self.client.browser.new_context(
                user_agent=current_ua,
                proxy=current_proxy
            )
            page = await context.new_page()

            try:
                logger.info(f"üßµ [–ü–æ—Ç—ñ–∫ #{index}] –ü–µ—Ä–µ—Ö—ñ–¥: {url} | Proxy: {proxy_label}")

                # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∏
                await page.goto(url, wait_until="domcontentloaded", timeout=60000)

                # --- –ï–ú–£–õ–Ø–¶–Ü–Ø –ü–û–í–ï–î–Ü–ù–ö–ò –õ–Æ–î–ò–ù–ò ---
                # 80% —à–∞–Ω—Å, —â–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á –ø–æ–≤–æ—Ä—É—à–∏—Ç—å –º–∏—à–∫–æ—é
                if random.random() < 0.8:
                    await human_mouse_move(page)

                # 60% —à–∞–Ω—Å, —â–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á –ø—Ä–æ–∫—Ä—É—Ç–∏—Ç—å —Å—Ç–æ—Ä—ñ–Ω–∫—É –≤–Ω–∏–∑ (–≤–∞–∂–ª–∏–≤–æ –¥–ª—è Lazy Load)
                if random.random() < 0.6:
                    await smooth_scroll(page)
                    # –ü—ñ—Å–ª—è —Å–∫—Ä–æ–ª—É —â–µ —Ç—Ä–æ—Ö–∏ —Ä—É—Ö–∞—î–º–æ –º–∏—à–µ—é, –Ω—ñ–±–∏ —á–∏—Ç–∞—î–º–æ –∑–Ω–∏–∑—É
                    await human_mouse_move(page)
                # --------------------------------

                # –í–ª–∞—Å–Ω–µ –ø–∞—Ä—Å–∏–Ω–≥
                new_items = await self.parser.parse_quotes(page)

                async with self._lock:
                    self._update_results(new_items)
                    count = len(self.results)

                logger.success(f"‚úÖ [–ü–æ—Ç—ñ–∫ #{index}] –£—Å–ø—ñ—à–Ω–æ –∑—ñ–±—Ä–∞–Ω–æ. –í –±–∞–∑—ñ: {count}")

                # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞ –ø–∞—É–∑–∞ –ø—ñ—Å–ª—è —Ä–æ–±–æ—Ç–∏
                min_d = BASE_DELAY[0] * self.concurrency if len(PROXY_LIST) <= 1 else BASE_DELAY[0]
                max_d = BASE_DELAY[1] * self.concurrency if len(PROXY_LIST) <= 1 else BASE_DELAY[1]
                await human_delay(min_d, max_d)

            except Exception as e:
                logger.error(f"‚ùå [–ü–æ—Ç—ñ–∫ #{index}] –ü–æ–º–∏–ª–∫–∞ –Ω–∞ {url}: {e}")
            finally:
                await context.close()

    async def run(self, urls: list[str]):
        """–ó–∞–ø—É—Å–∫ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏"""
        await self.client.start()
        semaphore = asyncio.Semaphore(self.concurrency)

        try:
            tasks = [self.scrape_page(semaphore, url, i + 1) for i, url in enumerate(urls)]
            await asyncio.gather(*tasks)

            logger.info(f"üèÅ –°–∫—Ä–∞–ø—ñ–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –†–∞–∑–æ–º –∑—ñ–±—Ä–∞–Ω–æ: {len(self.results)}")
            return self.results
        finally:
            await self.client.stop()

    def _update_results(self, new_items: list[QuoteModel]):
        existing_texts = {item.text for item in self.results}
        for item in new_items:
            if item.text not in existing_texts and len(self.results) < self.max_items:
                self.results.append(item)